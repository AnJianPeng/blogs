---
title: "Google File System"
date: 2024-02-18
---
# Introduction
**Google File System** is a distributed file system developed by Google for its own storage needs. GFS provides fault tolerance and high aggregated performance to a large number of clients while running on inexpensive commodity hardware.

GFS was driven by key observations of Google's application workloads and based on the assumptions:
1. Fault is common in a distributed system consisting of hundreds or even thousands of commodity machines
2. Files are huge, usually 100MB or larger in size (as of 2003).
3. In file operations, appending new data is much more common than overriding existing data. One typical scenarios is that files are used as producer-consumer queues.

Compared to academic papers, GFS is specail in 
1. Vast data in a big system with 100K+ machines
2. highlight the needs of throughput and performance while easing the consistency
3. Single master works fine in real-world while academia explores mutliple-master practice

# Distributed Storage System
## Motivation & Challenges
Start for performance

=> Sharding data with thousands of machines

=> Fault (machine could fail, especially with many machiens)

=> Fault tolerance (Replication)

=> Inconsitency between replication

=> Stronger consistency with "low performance"

## Bad Replication Designs
**Case 1** Clients responsible for sending requests towards replicas to update records

 *Caveat* No way to enforce the sequence of requests because requests can arrive in different order due to network (undefined data)

## Architecture
![GFS-Architecture](/blogs/images/gfs-architecture.png)

A GFS cluster consists of 1. a single active *master* sever (plus standby master replicas) 2. mutiple *chunkservers*. Also, it is accessed by *clients*.

The *master* only includes file system metdata while *chunkservers* store the real file data. The *master* is the bottleneck of the system, and to limit its overload, only metadata operations (control messages) go through it while data-bearing operations (data messages) are all handled by *chunkservers*.

*Chunkservers* will periodically send HEARTBEAT messages to *master*, reporting its own status. In the reponse of HEATBEAT, *master* also assign the lease on a chunk towards a *chunkserver*, known as *primary chunkserver* of a chunk.

## Details in Master Server
| Metadata | Fields | Volatility |
| --- | --- | --- |
| file name | array of CH(chunk handles) | Non-Volatile |
| chunk handle | array of CS(chunk servers) | Volatile |
| | version # | Non-Volatile |
| | primay chunk | Volatile |
| | lease expiration | Volatile |

**Persistence of Metadata**
No-volatile metadata in *master* is stored with the help of **operation logs**. Log is efficient since the write is sequential. To support fault tolerance on logs, it is replicated on multiple replicas. *Master* replies a client operation only after flusing the logs to disk locally and remotely.

To avoid long startup time of relaying all logs, *master* will checkpiont its state periodically.

## File & Chunk
Files in GFS are stored in chunks and by default **3 replicas**. There is a *primary* replica to determine the mutation order in writes. The chunk size in GFS is 64MB.

With large chunk size, it reduces the size of metadata in *master* as well as the interactions between *master*.

# Reads in GFS
```
string Read(fileName, offset)
```
1. client (fileName, offset)=> *master*
2. Master (CH, array of CS)=> Client
    * Clients cache the response 
3. Client (Data request)=> CS (one replica, mostly the closet one)

Since clients cache chunk metadata, they may read from a stale replica. However, it will refresh after the cache timeout.

# Writes in GFS
```
void Append(fileName, data)
```
## Chunk Version Number (CV#)
 Whenever *master* grants a new lease on a chunk, it increments CV#. The operations between clients and chunkservers will verify the CV# first before proceeding. 

## Identify primary in master
The client asks the master which chunkservers holds the current lease for a chunk and cache the metadata for further usage. If master doesn't know the primary of a chunk, it needs the following steps to assign:

1. Find up-to-date replicas
2. Pick Primary, Secondary
3. Increment CV#
4. Master (CV#)=> Primary, Secondary
    * Primary with lease expiration
    * Primary and secondary ack themselves
5. Master persists CV#

> We can't rely on CS's largest CV# to restore Master's V#, because not all CS are necessarily available when Master reboots.

## Writes
1. Client (Data) => replicas (primay + secondary)
    * Replicas store the data in memory cache
2. Client (Write Reqeust) => Primary replicas
3. **Primary assigns consecutive serial numbers to concurrent mutations**
4. Primary stores data locally
5. Primary (Write Request) => Other replicas
6. Secondary (Write Status) => Primary
7. IF
    * all replicas succeed, Primary ("Yes")=> Client
    * Otherwise, Primary ("No")=> Client

### Data Flow
During the above step 1, GFS uses a simple but brilliant way to fully utilize each machines's network bandwidth. When writing data to *chunkservers*' memory cache, each node picks the "closest" but not written machine in th network topology.

Also, the writes can be pipelined. Once a machine starts receiving the data, it can forward the data to the next machine.

## Case Study
**Case 1** Append request of a record failed because two replicas wrote the data but one replica failed.

The file will be inconsistent. GFS relies on clients' retry to finalize the writes until the write succeeds. It means there could be duplicates of the record.

**Case 2** Master thinks primary is dead due to the loss of HEARTBEAT.

If master designates a primary in such case, it causes "split brain" because the old primary could be still alive and serving requests. Thus, master needs to wait for the old primary's lease to expire before designating a new primary.

# Consistency Model
GFS provides a relaxed consistency model where replicas can be inconsistent and have duplicates from the same write when failures happen. It relies on the application logic for strong consistency:

1. Applications can identify and discard extra padding and record fragments using **checksums**.
2. Applications can filter out duplicates of recoreds by unique identifiers.



# Limitations
It is single master. As the evolvement of distributed storage system, this design could suffer from:
* Run out of RAM in Master since it needs to hold metadata for all files. 
* Throughput limitation on the number of connected clients to Master
* Manual repair is needed when master machine permanently failed.