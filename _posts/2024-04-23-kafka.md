---
title: "Kafka Design Introduction"
date: 2024-04-27
---

# Introduction
High throughput
Large data backlogs
Low lantency
Partition
Fault Tolerance

# Design
## Persistence
## Efficiency
small I/O operations, excessive byte copying

Batching - Amortize the network roundtrip cost by grouping messages in network requests
Larger network packets, larger sequential disk operations, contigugous memory blocks, smooth bursts of messages

Sendfile - avoid byte copying; pagecache -> socket

Batch Compression - Producer compress a batch of messages -> broker validates the batch -> Broker persist the batch in comprepssion form -> Consumer get compressed batches -> Consumer decompress

## Producer
Load balancing - Any Kafka nodes can answer metadata about leader of topics; Client controls which partition to publish messages; Support semantic partitioning
Asynchronous send - Accumulate messages before sending batches out asychronously

## Consumer
where data is pushed to the broker from the producer and pulled from the broker by the consumer

**Pull based system**

Pros - 1. Consumer controls its own pace, no overwhelming 2. Allow aggresive batching of data sent to the consumer
Cons - 1. Busy waiting (long poll mitigates it)

**Consumer Position**

In Brokers
Cons - 1. Complicated with broker management because message/acknowledgement could be lost/unordered in network
In Clients
Pros - 1. Small position metadata for each consumer group

**Offline Data Load**

Scalable persistence in Kafka allows bulk-load data into an offline system for further analysis, like Hadoop or a relational data warehouse.

**Static membership**

## Message Delivery Semantics
| Semantic Type | Description | Implementation |
| --- | --- | --- |
| At most once | Messages may be lost but are never redelivered | Failed messages may not be processed
| At least once | Messages are never lost but may be redelivered | Retry the failed messages could cause redelivery
| Exactly once | Each message is delivered once and only once | Idempotent message redelivery with unique ID check

After 0.11.0.0, Kafka supports transaction-like semantics when sending messages to multiple topics.
Users can ease guarantees based on their needs.

### How to achieve exactly once semantic?
**Scenario 1 - Read from a Kafka topic-A and write response to another topic-B**

The consumer position in topic-A is stored as a message in topic-C. Combining the write of consumer posistion and response into a Kafka transaction under "read_commited" isolation level.

**Scenario 2 - Read from a Kafka topic-A and write response to an external system**

1. Store the consumer position in topic-A in the external system and use the transactional semantics there.
2. Two-phase commit

## Log Compaction
Log compaction ensures that Kafka will always retain the last known value for each message key within the log of data for a single topic partition. In the following example, *bill@gmail.com* will be the value of the key *123*.
```
123 => bill@microsoft.com

123 => bill@gatesfoundation.org

123 => bill@gmail.com
```
This feature guarantees that Kafka keeps the final value for every key. Downstream consumers can restore the stat from the topic without retaining a complete log of all changes.


## Replication
| Node Type | Description | Health condition | 
| --- | --- | --- |
| Controller | a pecial node in Kafka for managing the registration of brokers | |
| Leaders | * All writes of a partition come to its leader <br> * Reads of the partition can go to its leader or followers | Timely heartbeats towards the controller |
| Followers | Reads of the partition can go to its leader or followers | 1. Timely heatbeats towards the controller <br> 2. Followers' log should not fall "too far" behind the leader |

A message is considered committed when all replicas in the ISR for that partition have applied it to their log.

### Quorum & Majority Vote
> If you choose the number of acknowledgements required and the number of logs that must be compared to elect a leader such that there is guaranteed to be an overlap, then this is called a Quorum.

> A common approach to this tradeoff is to use a majority vote for both the commit decision and the leader election.

Pros - The majority vote approach is dependent on the fastest servers, benefiting the latency.
Cons - The majority vote approach doesn't tolerate many server failures. 2x replicas only tolerate a single server failure but 4x replicas costs more throughput and more disk space.
Observation - The majority vote approach is more common for key configuration storage but NOT data storage.

### Kafka's Way
1. A message is considered committed when all replicas in the ISR for that partition have applied it to their log. Because of this, any replica in the ISR is eligible to be elected leader.
2. Another important design distinction is that Kafka does not require that crashed nodes recover with all their data intact.  Our protocol for allowing a replica to rejoin the ISR ensures that before rejoining, it must fully re-sync again even if it lost unflushed data in its crash.
Pros - With this ISR model and *f+1* replicas, a Kafka topic can tolerate *f* failures without losing committed messages.
Cons - Sacrifying latency since all replicas need to apply the changes.


